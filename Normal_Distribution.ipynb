{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad8ae8a",
   "metadata": {},
   "source": [
    "# Normal Distribution notebook\n",
    "\n",
    "### Student: Lais Coletta \n",
    "\n",
    "***\n",
    "\n",
    "## Table of content\n",
    "***\n",
    "\n",
    "   1. [What is Normal Distribution](#What-is-Normal-Distribution) \n",
    "   2. [Continuous vs. Discrete distribution](#Continuous-vs.-Discrete-distribution)\n",
    "   \n",
    "   \n",
    "   3. [Standard Normal Distribution](#Standard-Normal-Distribution)\n",
    "   4. [Sample Size and Normal Distribution](Sample-Size-and-Normal-Distribution)\n",
    "   5. [Bell-Curve](Bell-Curve)\n",
    "   6. [The empirical rule](The-empirical-rule)\\\n",
    "       6.1. [Testing the Empirical rule in Python](Testing-the-Empirical-rule-in-Python)\n",
    "   7. [Skewerness](Skewerness)\n",
    "   8. [Normality tests](Normality-tests)\\\n",
    "       8.1. [Anderson-Darling test](Anderson-Darling-test)\\\n",
    "       8.2. [Shapiro-Wilk test](Shapiro-Wilk-test)\\\n",
    "       8.3. [Kolmogorov-Smirnov test](Kolmogorov-Smirnov-test)\n",
    "   9. [Real-life example of Normal Distribution](Real-life-example-of-Normal-Distribution)\n",
    "   10. [Conclusion](Conclusion)\n",
    "   11. [References](References)\n",
    "    \n",
    "\n",
    "\n",
    "## What is Normal Distribution\n",
    "***\n",
    "\n",
    "A probability distribution is a statistical function that describes the likelihood of obtaining the possible values that a random variable can take. Normal Distribution is one of the most basic probability distribution types. Also known as the Gaussian distribution, it is a continuous probability distribution that is symmetrical around the mean, with a bell-shaped curve. It also demonstrates that data close to the mean occurs more frequently than data far from it. \n",
    "\n",
    "Normal Distribution is described by its mean (which is the average value of the data or <b>$\\mu$</b>) and standard deviation (which measures the spread of the data around the mean or <b>$\\sigma$</b>). As the standard deviation increases, the curve becomes flatter and more spread out, while a smaller standard deviation results in a taller, more narrow curve. The probability density function (PDF) of the normal distribution is given by:\n",
    "\n",
    "$$ f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{ -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 } $$\n",
    "\n",
    "The function $f(x \\mid \\mu, \\sigma)$ represents the probability density at a particular value of $x$ given the mean $\\mu$ and standard deviation $\\sigma$. The term $\\frac{1}{\\sigma \\sqrt{2\\pi}}$ is a normalizing constant that ensures that the area under the curve is equal to 1, and the term $e^{ -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 }$ is the probability density at $x$.\n",
    "\n",
    "The normal distribution is often used to model real-valued random variables because it has several properties, it has applications in many fields, including finance, biology, and engineering. For example, it is often used to model the distribution of financial returns, the distribution of heights in a population, and the distribution of errors in measurements. Because normally distributed variables are so common, many statistical tests are designed for normally distributed populations.\n",
    "\n",
    "Understanding the properties of normal distributions means you can use infer statistical numbers to compare different groups and make estimates about populations using samples such as discussed in our numpy.random modules.\n",
    "\n",
    "## Continuous vs. Discrete distribution\n",
    "***\n",
    "\n",
    "There are two main types of probability distributions: continuous and discrete.\n",
    "\n",
    "A continuous probability distribution is a distribution that is defined over a continuous range of values, such as all possible values on the real number line. Continuous distributions are often used to model phenomena that can take on any value within a certain range, such as the height of a person or the time it takes for a chemical reaction to occur. Examples of continuous probability distributions include the normal distribution, the uniform distribution, and the exponential distribution.\n",
    "\n",
    "A discrete probability distribution is a distribution that is defined over a set of discrete values, such as integers or categories. Discrete distributions are often used to model phenomena that can only take on a limited set of values, such as the number of heads that result from flipping a coin or the number of defects in a manufactured product. Examples of discrete probability distributions include the binomial distribution, the Poisson distribution, and the geometric distribution.\n",
    "\n",
    "It is important to note that probability distributions can be either continuous or discrete, depending on the nature of the random event being modeled. For example, the height of a person is a continuous variable, so a continuous probability distribution would be used to model the distribution of heights in a population. On the other hand, the number of heads that result from flipping a coin is a discrete variable, so a discrete probability distribution would be used to model the probability of different numbers of heads occurring.\n",
    "\n",
    "Here is a graph taken from the website [databasetown.com](https://databasetown.com/types-probability-distribution-characteristics-examples/) which shows the difference between concrete and discrete and examples of types of distributions:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe4b4f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://databasetown.com/wp-content/uploads/2019/08/different-types-of-probability-distributioncharacteristics-and-examples-1024x1024.jpg\" style=\"height:400px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4881cfe",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution\n",
    "***\n",
    "\n",
    "The standard normal distribution is a normal distribution with a mean of 0 and a standard deviation of 1. It is also known as the <b>\"z-distribution\" </b>because it is often used to standardize data by converting it to \"z-scores.\"\n",
    "\n",
    "A z-score is a measure of how many standard deviations a value is away from the mean. For example, a z-score of 1 indicates that a value is one standard deviation above the mean, while a z-score of -2 indicates that a value is two standard deviations below the mean.\n",
    "\n",
    "The standard normal distribution is a useful reference distribution because it allows you to compare data from different distributions on the same scale. For example, if you have data from two different populations with different means and standard deviations, you can convert the data to z-scores using the mean and standard deviation of each population, and then compare the data on the same scale.\n",
    "\n",
    "Here is an example of a standard normal distribution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c739023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05758770894870196\n",
      "1.0297522009099795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate 1000 random numbers from the standard normal distribution\n",
    "random_numbers = norm.rvs(size=1000, loc=0, scale=1)\n",
    "\n",
    "# Print the mean and standard deviation of the generated numbers\n",
    "print(np.mean(random_numbers))\n",
    "print(np.std(random_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb12f2",
   "metadata": {},
   "source": [
    "The <b>rvs </b>function generates random numbers from the specified distribution, and the size argument determines how many numbers to generate. The <b>loc</b> argument specifies the mean of the distribution, and the scale argument specifies the standard deviation.\n",
    "\n",
    "In this example, the <b>norm.rvs</b> function generates 1000 random numbers from a normal distribution with a mean of 0 and a standard deviation of 1. The <b>np.mean and np.std </b>functions are used to calculate the mean and standard deviation of the generated numbers, which should be close to the specified values of 0 and 1, respectively.\n",
    "\n",
    "Let's use scipy.stats module to plot the standard normal distribution and visualize its shape. The following code generates random numbers from the standard normal distribution and plots a histogram of the numbers using the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8502777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 1000 random numbers from the standard normal distribution\n",
    "random_numbers = norm.rvs(size=1000, loc=0, scale=1)\n",
    "\n",
    "# Plot a histogram of the generated numbers\n",
    "plt.hist(random_numbers, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2255ee4",
   "metadata": {},
   "source": [
    "This code generates 1000 random numbers from the standard normal distribution and plots a histogram of the numbers using 50 bins. The resulting plot should show a bell-shaped curve that is symmetrical around the mean, which is 0 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074754ef",
   "metadata": {},
   "source": [
    "## Sample size and normal distribution\n",
    "***\n",
    "\n",
    "In the Normal distribution lecture in this module we could see the relationship between sample size and normal distribution. Using the article 'Normal Distribution' published in the website link [National Library of Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/#:~:text=Sample%20size%20has%20a%20significant,result%20in%20a%20normal%20curve) written by Jogikalmat Krithikadatta, we will explain this correlation in more detail.\n",
    "\n",
    "According to the author, <i>\"Sample size has a significant effect on sample distribution. It is often observed that small sample size results in non-normal distribution. This is a result of inadequate estimation of the dispersion of the data, and the frequency distribution does not result in a normal curve</i>.\n",
    "\n",
    "As a general rule, the larger the sample size, the more accurate and precise the estimates derived from the sample will be. This is because a larger sample size allows for a more representative and comprehensive representation of the underlying population, which in turn leads to more stable and reliable estimates.\n",
    "\n",
    "On the other hand, smaller sample sizes may result in less accurate and less precise estimates, as they may not adequately capture the true characteristics of the population. This can be particularly problematic when the underlying population is not normally distributed, as the assumptions of normality may not hold for smaller sample sizes.\n",
    "\n",
    "The appropriate sample size for a given study will depend on a variety of factors, including the size of the underlying population, the level of precision desired, and the resources available for collecting and analyzing the data.\n",
    "\n",
    "To understand the effect of sample size on distribution, the author Jogikalmat Krithikadatta uses the examples of the research \"What is the shear bond strength of self-etch adhesive to dentin?\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55617daf",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/bin/JCD-17-96-g001.jpg\" style=\"height:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf7c8a",
   "metadata": {},
   "source": [
    "As illustrated above, the graph does not conform to the bell curve when the sample size is 10, 15 or 20. When the sample size increases to 25 (1d), the distribution is beginning to conform to the normal curve and becomes normally distributed when sample size is 30 (1e). \n",
    "\n",
    "There is a tendency to assume that the normalcy would be better with very large sample size. However, it is observed by the author that the data shows normal distribution at n = 30 (1e) and the distribution remains the same when the sample size is 120 (1f). \n",
    "\n",
    "When we look at the mean and standard deviation for different sample sizes, it can be noted that the mean varies from 35 to 32 MPa between n = 10 and n = 25, but stabilizes at 33.3 MPa when n = 30. However, the standard deviation is gradually decreasing from 7.57 to 5.04 with an increase in sample size. Hence the shape of the normal distribution is a function of SD. The shape is broader and flatter when standart deviation is high and narrower when it is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55773722",
   "metadata": {},
   "source": [
    "## \"Bell Curve\"\n",
    "***\n",
    "\n",
    "A bell curve, also known as a normal distribution curve or Gaussian curve, is a symmetrical, bell-shaped curve that is commonly used in statistical analysis to model a wide variety of phenomena.\n",
    "\n",
    "As explained above, the shape of the bell curve is determined by the mean and the standard deviation. As the mean increases, the curve shifts to the right. As the standard deviation increases, <b>the curve becomes flatter and more spread out, while a smaller standard deviation results in a taller, more narrow curve</b>.\n",
    "\n",
    "The bell curve has several important properties that make it useful in statistical analysis. One of these properties is that a large portion of the data is concentrated around the mean, with fewer and fewer data points as you move further away from the mean. This property is often referred to as the \"68-95-99.7\" or Empirical rule. Other important properties of a normal distribution are:\n",
    "\n",
    "* The mean, mode, and median are all equal.\n",
    "* The total area under the curve is equal to 1\n",
    "* The curve is symmetric around the mean\n",
    "\n",
    "## The Empirical rule\n",
    "***\n",
    "\n",
    "The empirical rule, also known as the 68-95-99.7 rule, represents the percentages of values within an interval for a normal distribution. It states that approximately:\n",
    "\n",
    "- 68% of the data will fall within one standard deviation of the mean.\n",
    "- 95% of the data will fall within two standard deviations of the mean.\n",
    "- 99.7% will fall within three standard deviations of the mean.\n",
    "\n",
    "Here is a graph representation of the Empirical rule from the website [builtin.com](https://builtin.com/data-science/empirical-rule):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba98af",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_empirical-rule.jpg\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3025c",
   "metadata": {},
   "source": [
    "### Testing the Empirical rule in Python\n",
    "\n",
    "Let's use python to calculate the percentage of data that falls within a certain number of standard deviations of the mean for a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5a1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6826894921370859\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Calculate the percentage of data within one standard deviation of the mean for a normal distribution with mean 100 and standard deviation 15\n",
    "mean = 100\n",
    "std = 15\n",
    "lower_bound = mean - std\n",
    "upper_bound = mean + std\n",
    "# The cumulative density function (CDF) is the probability that a value is less than or equal to a given value.\n",
    "percentage = norm.cdf(upper_bound, loc=mean, scale=std) - norm.cdf(lower_bound, loc=mean, scale=std)\n",
    "\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d718f",
   "metadata": {},
   "source": [
    "This code calculates the percentage of data within one standard deviation of the mean by subtracting the CDF at the lower bound from the CDF at the upper bound. The norm.cdf function calculates the CDF of the normal distribution at a given value, and the loc and scale arguments specify the mean and standard deviation of the distribution.\n",
    "\n",
    "Now, let's calculate the percentage of data within two standard deviations of the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98ec9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544997361036416\n"
     ]
    }
   ],
   "source": [
    "mean = 100\n",
    "std = 15\n",
    "lower_bound = mean - 2 * std\n",
    "upper_bound = mean + 2 * std\n",
    "percentage = norm.cdf(upper_bound, loc=mean, scale=std) - norm.cdf(lower_bound, loc=mean, scale=std)\n",
    "\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80083b35",
   "metadata": {},
   "source": [
    "The calculation of the percentage data within three standard deviations is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11e88c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973002039367398\n"
     ]
    }
   ],
   "source": [
    "mean = 100\n",
    "std = 15\n",
    "lower_bound = mean - 3 * std\n",
    "upper_bound = mean + 3 * std\n",
    "percentage = norm.cdf(upper_bound, loc=mean, scale=std) - norm.cdf(lower_bound, loc=mean, scale=std)\n",
    "\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1e577",
   "metadata": {},
   "source": [
    "These results confirm the graph above and shows that <b>68%</b> of the data will fall within one standard deviation of the mean, <b>95% </b>of the data will fall within two standard deviations of the mean and <b>99.7%</b> will fall within three standard deviations of the mean. Those percentages are equally distributed as shown in the graph bellow copied from [builtin.com](https://builtin.com/data-science/empirical-rule):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a4c03",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/8_empirical-rule_0.jpg\" style=\"height:250px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013f238",
   "metadata": {},
   "source": [
    "## Skewerness\n",
    "***\n",
    "\n",
    "According to [medium.com](https://yashowardhanshinde.medium.com/what-is-skewness-in-data-how-to-fix-skewed-data-in-python-a792e98c0fa6#:~:text=Skewness%20is%20an%20asymmetry%20in,called%20as%20Negatively%2DSkewed%20data.): <i>\"Skewness is an asymmetry in the distribution of data as it does not show any kind of symmetry in continuous data\"</i>. \n",
    "\n",
    "If the distribution is symmetrical, the skewness is zero. If the distribution is skewed to the left (i.e., the tail on the left side is longer), the skewness is negative. If the distribution is skewed to the right (i.e., the tail on the right side is longer), the skewness is positive.\n",
    "\n",
    "According to the website, there are 2 main methods to identify skewness in the data. The first is the Observational method and, the second is the Statistical method. Observational can be done easily by plotting a histogram and observing a few characteristics. As seen above, Normal distribution has a bell-shaped curve where both the ends (tail regions) taper equally with a peek at the centre of the distribution. In the other hand, left-skewness or negative skewness, the histogram shows the left part of the distribution with the peak shifted towards the right-hand side:\n",
    "\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/1306/1*oWh214Dm0mbDdoGAlVw_gg.webp\" style=\"height:250px\" align=\"center\"/>\n",
    "\n",
    "In the positive or left-skewned distribution, the peak shifted towards the left-hand side:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed63561",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/1314/1*oDRXZugvjxbDlxqJEwmJzA.webp\" style=\"height:250px\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Below we can see some of the types of probability distributions and how they represented in a graph according to the article [Understanding Probability Distribution in medium.com](https://medium.com/swlh/understanding-probability-distribution-b5c041f5d564):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c5a87",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/1400/1*nOMS0KgevT7YfqtfnhgXUg.webp\" style=\"height:400px\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729f6a6",
   "metadata": {},
   "source": [
    "## Normality tests\n",
    "***\n",
    "\n",
    "When analysing a dataset, there are several ways to determine whether the data is normally distributed, or approximately normally distributed. According to the article [How to Test for Normality in Python](https://www.statology.org/normality-test-python/) common methods include:\n",
    "\n",
    "- Visual inspection: One simple way to check for normality is to plot the data using a histogram or a normal quantile plot. A histogram is a graph that shows the frequency distribution of a set of data, and a normal quantile plot is a graph that compares the data to a theoretical normal distribution. If the data are approximately normally distributed, the histogram or normal quantile plot should roughly follow a bell-shaped curve.\n",
    "\n",
    "- Statistical tests: There are also several statistical tests that can be used to formally test whether a dataset is normally distributed. Some commonly used tests include the Anderson-Darling test, the Shapiro-Wilk test, and the Kolmogorov-Smirnov test. These tests compare the data to a theoretical normal distribution and provide a measure of how well the data fit the normal curve.\n",
    "\n",
    "It is important to keep in mind that no single method is foolproof, and it may be necessary to use a combination of methods to determine whether a dataset is approximately normally distributed. In addition, it is important to recognize that normality is not always a necessary assumption for statistical analysis, and other types of distributions may be just as appropriate for certain types of data and research questions.\n",
    "\n",
    "#### Anderson-Darling test\n",
    "\n",
    "The Anderson-Darling test is a statistical test used to determine whether a sample of data is drawn from a population that follows a particular distribution, such as the normal distribution. It is based on the Anderson-Darling statistic, which measures the distance between the observed data and the expected values under the specified distribution.\n",
    "\n",
    "To perform the Anderson-Darling test, we first calculate the Anderson-Darling statistic for the sample. This is done by comparing the sample data to the cumulative distribution function (CDF) of the assumed distribution, and then summing the resulting squared differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da6623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AndersonResult(statistic=0.13676646631470035, critical_values=array([0.507, 0.578, 0.693, 0.808, 0.961]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import anderson\n",
    "import numpy as np\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "result = anderson(data, dist='norm')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0c696",
   "metadata": {},
   "source": [
    "This will output the Anderson-Darling statistic and the critical values for the test. The critical values are used to determine whether the sample data is significantly different from the hypothesized normal distribution (in this case). If the Anderson-Darling statistic is larger than the critical value, it indicates that the sample data is significantly different from the hypothesized normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c75d7",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk test\n",
    "\n",
    "This test calculates a statistic, known as the W-statistic, which is used to assess the fit of the sample data to a normal distribution. If the W-statistic is small, it indicates that the sample data is significantly different from a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3547918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722883105278015 0.9135605096817017\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "w, p = shapiro(data)\n",
    "print(w, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1baaf18",
   "metadata": {},
   "source": [
    "This will output the W-statistic and the p-value for the test. The p-value is used to determine the significance of the result. If the p-value is small (e.g., less than 0.05), it indicates that the sample data is significantly different from a normal distribution.\n",
    "\n",
    "It's important to note that the Shapiro-Wilk test is sensitive to sample size, so it is more powerful for smaller sample sizes (n < 50). For larger sample sizes, other tests, such as the Anderson-Darling test or the Kolmogorov-Smirnov test, may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98831f",
   "metadata": {},
   "source": [
    "#### Kolmogorov-Smirnov test\n",
    "\n",
    "According to [geeksforgeeks.com](https://www.geeksforgeeks.org/kolmogorov-smirnov-test-ks-test/), the Kolmogorov-Smirnov (KS), known as the KS statistic, is a very efficient way to determine if two samples are significantly different from each other. It <i>\"quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples\"</i>. Therefore, if the KS statistic is large, it indicates that the sample data is significantly different from the hypothesized distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fd440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0.07936507936507936\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "data1 = [1, 2, 3, 4, 5]\n",
    "data2 = [5, 6, 7, 8, 9]\n",
    "ks_statistic, p_value = ks_2samp(data1, data2)\n",
    "print(ks_statistic, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7f7c7",
   "metadata": {},
   "source": [
    "This will output the KS statistic and the p-value for the test. The p-value is used to determine the significance of the result. If the p-value is small (e.g., less than 0.05), it indicates that the sample data is significantly different from the hypothesized distribution.\n",
    "\n",
    "It's important to note that the Kolmogorov-Smirnov test is sensitive to the overall distribution of the data, rather than specific parameters such as the mean or variance. It is a non-parametric test, meaning it does not assume that the data follows a particular distribution. As a result, it can be used to test the fit of the data to a wide range of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7004d96",
   "metadata": {},
   "source": [
    "## Real-life example of Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237e216",
   "metadata": {},
   "source": [
    "A fair rolling of dice is an example of a data that is normally distributed. Below, we can see that when a dice is rolled 1000 times, chances to get ‘1’ are 15-18% and if we roll the dice 1000 times, the chances to get ‘1’ is, again, the same, which averages to 16.7% (1/6). If we roll two dices simultaneously, there are 36 possible combinations. The probability of rolling ‘1’ (with six possible combinations) again averages to around 16.7%, i.e., (6/36). More the number of dices more elaborate will be the normal distribution graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cace3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of rolling a 6: 0.17\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Set the number of dice rolls\n",
    "num_rolls = 1000\n",
    "\n",
    "# Roll the dice and count the number of times we get a 6\n",
    "num_sixes = 0\n",
    "for _ in range(num_rolls):\n",
    "    if random.randint(1, 6) == 6:\n",
    "        num_sixes += 1\n",
    "\n",
    "# Calculate the probability of rolling a 6\n",
    "probability = num_sixes / num_rolls\n",
    "print(f\"Probability of rolling a 6: {probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdac0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "# Roll the dice and calculate the mean for each sample\n",
    "means = []\n",
    "for _ in range(num_samples):\n",
    "    rolls = [random.randint(1, 6) for _ in range(num_rolls)]\n",
    "    means.append(np.mean(rolls))\n",
    "\n",
    "# Plot the distribution of means using a histogram\n",
    "plt.hist(means, bins=20, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aea35",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "A normal distribution is a type of probability distribution that is symmetrical around the mean, with a bell-shaped curve. It is also known as a Gaussian distribution. In a normal distribution, the mean, median, and mode are all equal, and the distribution has a uniform shape.\n",
    "\n",
    "One of the key properties of a normal distribution is that about 68% of the data falls within one standard deviation of the mean, about 95% falls within two standard deviations, and about 99.7% falls within three standard deviations. This property is known as the 68-95-99.7 rule or the empirical rule.\n",
    "\n",
    "It is important to note that not all data follows a normal distribution, and it is important to check the distribution of your data before assuming it is normally distributed and using statistical tests that assume a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb25342",
   "metadata": {},
   "source": [
    "## References: \n",
    "\n",
    "Normal Distribution definitions - https://en.wikipedia.org/wiki/Normal_distribution\n",
    "\n",
    "Standart Normal Distribution explained: https://www.mathsisfun.com/data/standard-normal-distribution.html\n",
    "\n",
    "Gaussian distribution on Wikipedia: https://en.wikipedia.org/wiki/Gaussian_distribution\n",
    "\n",
    "Normal Distribution Bell Curve: https://www.askpython.com/python/normal-distribution\n",
    "\n",
    "Empirical rule graph and explanation references: https://builtin.com/data-science/empirical-rule\n",
    "\n",
    "Article in the National Library of medicine about sample size: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915399/#:~:text=Sample%20size%20has%20a%20significant,result%20in%20a%20normal%20curve.\n",
    "\n",
    "Skewness definition and examples: https://yashowardhanshinde.medium.com/what-is-skewness-in-data-how-to-fix-skewed-data-in-python-a792e98c0fa6#:~:text=Skewness%20is%20an%20asymmetry%20in,called%20as%20Negatively%2DSkewed%20data.\n",
    "\n",
    "Understanding Probability Distribution: https://medium.com/swlh/understanding-probability-distribution-b5c041f5d564\n",
    "\n",
    "How to Test for Normality in Python: https://www.statology.org/normality-test-python/\n",
    "\n",
    "NORMALITY TEST definition and Shapiro-wilk test: https://www.isixsigma.com/dictionary/normality-test/\n",
    "\n",
    "Anderson Darling test: https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\n",
    "\n",
    "Shapiro-Wilk test: https://www.statisticshowto.com/shapiro-wilk-test/\n",
    "\n",
    "Kolmogorov-Smirnov test: https://www.geeksforgeeks.org/kolmogorov-smirnov-test-ks-test/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
